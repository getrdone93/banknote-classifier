{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook uses the UCI banknote dataset\n",
    "- https://archive.ics.uci.edu/ml/datasets/banknote+authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 55%, positive: 45%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.445785</td>\n",
       "      <td>1.909039</td>\n",
       "      <td>1.413578</td>\n",
       "      <td>-1.168712</td>\n",
       "      <td>0.452522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.862906</td>\n",
       "      <td>5.868600</td>\n",
       "      <td>4.328365</td>\n",
       "      <td>2.085877</td>\n",
       "      <td>0.497925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.786650</td>\n",
       "      <td>-1.627000</td>\n",
       "      <td>-1.545600</td>\n",
       "      <td>-2.393100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.518735</td>\n",
       "      <td>2.334150</td>\n",
       "      <td>0.605495</td>\n",
       "      <td>-0.578890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.853250</td>\n",
       "      <td>6.796025</td>\n",
       "      <td>3.199800</td>\n",
       "      <td>0.403863</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance     skewness     curtosis      entropy        class\n",
       "count  1348.000000  1348.000000  1348.000000  1348.000000  1348.000000\n",
       "mean      0.445785     1.909039     1.413578    -1.168712     0.452522\n",
       "std       2.862906     5.868600     4.328365     2.085877     0.497925\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.786650    -1.627000    -1.545600    -2.393100     0.000000\n",
       "50%       0.518735     2.334150     0.605495    -0.578890     0.000000\n",
       "75%       2.853250     6.796025     3.199800     0.403863     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/data_banknote_authentication.csv')\n",
    "feats = list(data)[:-1]\n",
    "data.drop_duplicates(keep='first', inplace=True, ignore_index=True, subset=list(data)[:-1])\n",
    "neg, pos = [data.loc[data['class'] == arg] for arg in (0, 1)]\n",
    "total = len(data)\n",
    "print(\"negative: {:.0%}, positive: {:.0%}\".format(len(neg) / total, len(pos) / total))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity measure\n",
    "- Let $p_{nc}$ denote the density of a class $c$ in a given node $n$\n",
    "$$G(N_n) = \\sum\\limits_{c} p_{nc}(1-p_{nc})$$\n",
    "\n",
    "- Minimizing $G$ encourages the model to make splits such that the data is better separated. When $p_{nc}$ approaches 0, then less instances of $c_i$ exist in $n$ and more instances of $c_j$ exist in $n$. The converse is true as well, that is, when $p_{nc}$ approaches 1, then more instances of $c_j$ exist in $n$ and less instances of $c_i$ exist in $n$. When $p_{nc}$ approaches 0 or 1, then the summation term approaches 0 and the summation approaches 0. Thus, the model is encouraged to choose splits that further segregate the data.\n",
    "\n",
    "### Cost complexity pruning for binary tree\n",
    "- Cost complexity pruning is used to regularize the tree and prevent overfitting. Cost complexity pruning can be defined as\n",
    "$$V = C + \\alpha * S$$\n",
    "where $S$ is a sequence $N, N - 2, N - 4, N - 6, ... N - 2 * N_{int}$ and $N_{int}$ is the number of internal nodes in $N$, parameter $\\alpha \\geq 0$, and $C$ is a cumulative sum of the sequence $0, C_1, C_1, C_2, ... C_n$. $argmin (V)$ gives the appropriate number of prunes to perform. This can be seen by the fact that as $\\alpha$ grows, so does $S$. Therefore, more cost must be accrued to reduce the tree complexity and $argmin(V)$ grows accordingly.\n",
    "- The prune complexity of a single node can be defined as\n",
    "$$ P_c = W(t) - W(I_t) - W(J_t) $$\n",
    "where $t$ is some tree, $I$ and $J$ are children of $t$, and $W$ is the sample weighted impurity of the node. $C$ is a sequence of $P_c$ ordered from least costly to most, and the least costly nodes are pruned first until $argmin(V)$ is surpassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_ccp_alpha  rank_test_score  std_test_score  mean_test_score\n",
      "0            0.001                1        0.006191         0.985169\n",
      "1       0.00304167                2        0.007764         0.981457\n",
      "2       0.00508333                3        0.011463         0.977756\n",
      "3         0.007125                4        0.011602         0.975534\n",
      "4       0.00916667                5        0.016964         0.965163\n",
      "5        0.0112083                6        0.015249         0.957739\n",
      "6          0.01325                7        0.014021         0.955509\n",
      "7        0.0152917                8        0.008930         0.946595\n",
      "8        0.0173333                9        0.018816         0.938439\n",
      "9         0.019375               10        0.017344         0.929533\n",
      "11       0.0234583               11        0.015947         0.927311\n",
      "12          0.0255               11        0.015947         0.927311\n",
      "10       0.0214167               11        0.015947         0.927311\n",
      "13       0.0275417               14        0.014962         0.925824\n",
      "14       0.0295833               14        0.014962         0.925824\n",
      "15        0.031625               16        0.014121         0.924343\n",
      "16       0.0336667               16        0.014121         0.924343\n",
      "17       0.0357083               16        0.014121         0.924343\n",
      "18         0.03775               16        0.014121         0.924343\n",
      "19       0.0397917               20        0.016650         0.918417\n",
      "20       0.0418333               21        0.022334         0.899854\n",
      "23       0.0479583               22        0.016099         0.894669\n",
      "21        0.043875               22        0.016099         0.894669\n",
      "22       0.0459167               22        0.016099         0.894669\n",
      "24            0.05               22        0.016099         0.894669\n"
     ]
    }
   ],
   "source": [
    "#Create mapping and model\n",
    "xs, ys = np.asarray(data[list(data)[:-1]]), np.asarray(data[list(data)[-1]])\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "#CV multiple models, grid search over cost complexity\n",
    "param_space = {'ccp_alpha': np.linspace(0.001, 0.05, num=25)}\n",
    "search = GridSearchCV(estimator=model, param_grid=param_space, cv=5)\n",
    "search.fit(xs, ys)\n",
    "df = pd.DataFrame(search.cv_results_)[['param_ccp_alpha', 'rank_test_score', 'std_test_score', 'mean_test_score', 'params']]\\\n",
    "    .sort_values(by=['rank_test_score'])\n",
    "#print cross validation results sorted by rank_test_score\n",
    "print(df[list(df)[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ccp_alpha of 0.05 was used to see how the tree changes. Obviously, it is too aggresive and causes severe underfitting.\n",
    "- No visiblity on the trees used in CV. However, we can fit the trees again to the entire dataset during analysis and see how pruning affects the shape.\n",
    "- Accuracy measurements do not correspond one-to-one between succeeding trees and values in table, but they would be close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will be used to draw decision trees\n",
    "def graph(model, feats, plot_dir, name):\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, feature_names=feats, \n",
    "                                    class_names=['authentic', 'fraudulent'], \n",
    "                                    filled=True, rounded=True, special_characters=True, \n",
    "                                    leaves_parallel=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    out_path = os.path.join(plot_dir, name)\n",
    "    graph.render(out_path, format='png', cleanup=True)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"300\"\n",
       "            src=\"./graphs/overfit.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f21d56bc128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_model = tree.DecisionTreeClassifier(**df['params'].iloc[0])\n",
    "overfit_model.fit(xs, ys)\n",
    "graph(overfit_model, feats, './graphs', 'overfit')\n",
    "#Reminder: you can click on the iframe to zoom in\n",
    "IFrame(\"./graphs/overfit.png\", width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tree overfits the data. There are several buckets that have one sample in them and the tree is too complex. Thus, this tree will not generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"./graphs/underfit.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f21d56bce48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underfit_model = tree.DecisionTreeClassifier(**df['params'].iloc[-1])\n",
    "underfit_model.fit(xs, ys)\n",
    "graph(underfit_model, feats, './graphs', 'underfit')\n",
    "IFrame(\"./graphs/underfit.png\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing the last hyperparameter shows a tree that underfits because it is lacking in complexity. However, I am amazed to see that we can achieve roughly 88% - 91% with such a simple tree.\n",
    "- If such a simple tree achieves such high accuracy, then that must speak to the high degree of separability in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"./graphs/bestfit_model.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f21d54059e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestfit_model = tree.DecisionTreeClassifier(**df['params'].iloc[3])\n",
    "bestfit_model.fit(xs, ys)\n",
    "graph(bestfit_model, feats, './graphs', 'bestfit_model')\n",
    "IFrame(\"./graphs/bestfit_model.png\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tree at index 3 appears to be far less complex than the one at 0 and has no small buckets.\n",
    "- This tree has a higher variance than the tree at index 0, and looks to produce roughly 96% - 98% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
